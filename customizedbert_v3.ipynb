{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "customizedbert_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/f422661/google_qa/blob/master/customizedbert_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "KD6JKyGoW-3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "# for dirname, _, filenames in os.walk('..'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))\n",
        "\n",
        "# Any results you write to the current directory are saved as output."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz_vXPEMHfAL",
        "colab_type": "code",
        "outputId": "7e2aa6d6-0db4-4421-be6e-6ec740c483f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jan 12 09:47:57 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZhO3xxuXtwR",
        "colab_type": "code",
        "outputId": "a35d61e8-a7f9-4417-fde7-ca8198be85d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq49afG7bPLx",
        "colab_type": "code",
        "outputId": "147a8256-92eb-47ca-bb5e-b495c285e137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/10/aeefced99c8a59d828a92cc11d213e2743212d3641c87c82d61b035a7d5c/transformers-2.3.0-py3-none-any.whl (447kB)\n",
            "\r\u001b[K     |▊                               | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 4.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30kB 5.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 61kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 81kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 102kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 112kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 122kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 133kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 143kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 153kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 163kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 174kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 184kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 194kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 204kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 215kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 225kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 235kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 245kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 256kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 266kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 276kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 286kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 296kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 307kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 317kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 327kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 337kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 348kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 358kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 368kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 378kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 389kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 399kB 6.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 409kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 419kB 6.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 430kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 440kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 450kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 20.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=44dd909c217d153ff6921c3cfc6c7a8ea5bb7b4a5ad034727f18565661c55660\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 transformers-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgjmAqfmUxDO",
        "colab_type": "code",
        "outputId": "14ffa4b4-f092-4695-92fe-0890022234d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install flashtext"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flashtext\n",
            "  Downloading https://files.pythonhosted.org/packages/81/d8/2cd0656eae456d615c2f1efbcae8dfca2cb871a31f34ba8925aba47d5e09/flashtext-2.7.tar.gz\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9298 sha256=596eef1870acb42aae8dcfd5285e1fb9f09c393650c8c75da53cb824b43d0615\n",
            "  Stored in directory: /root/.cache/pip/wheels/37/db/d7/fe74f7cb8e5c3afed90fe6f4967c933a6f13d81ab6b3d3128c\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "",
        "_uuid": "",
        "id": "QHc6btamW-3h",
        "colab_type": "code",
        "outputId": "d2826da7-661f-46a4-f1e2-1de069d3ce03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import pickle\n",
        "import tensorflow.keras.backend as K\n",
        "import gc\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import torch\n",
        "from scipy.stats import spearmanr\n",
        "from flashtext import KeywordProcessor\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from transformers import AdamW,BertForSequenceClassification\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlYdOWB3W-3j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(\"/content/drive/My Drive/google-quest-challenge/sample_submission.csv\")\n",
        "test = pd.read_csv(\"/content/drive/My Drive/google-quest-challenge/test.csv\")\n",
        "train = pd.read_csv(\"/content/drive/My Drive/google-quest-challenge/train.csv\")\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIqdsDMrW-3l",
        "colab_type": "code",
        "outputId": "5853029e-daee-48cd-abe4-55bfbaa31154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print('train shape =', train.shape)\n",
        "print('test shape =', test.shape)\n",
        "\n",
        "output_categories = list(train.columns[11:])\n",
        "input_categories = list(train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', output_categories)\n",
        "print('\\ninput categories:\\n\\t', input_categories)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape = (6079, 41)\n",
            "test shape = (476, 11)\n",
            "\n",
            "output categories:\n",
            "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
            "\n",
            "input categories:\n",
            "\t ['question_title', 'question_body', 'answer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kN3He7eUd4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PUNCTS = {\n",
        "            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n",
        "            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n",
        "            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n",
        "            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n",
        "            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n",
        "            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n",
        "            '↑', 'º', '¯', '♫', '#'\n",
        "          }\n",
        "\n",
        "\n",
        "mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n",
        "\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
        "\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
        "\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n",
        "\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n",
        "\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n",
        "\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n",
        "\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n",
        "\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n",
        "\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n",
        "\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n",
        "\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n",
        "\n",
        "\n",
        "def clean_punct(text):\n",
        "  text = str(text)\n",
        "  for punct in PUNCTS:\n",
        "    text = text.replace(punct, ' {} '.format(punct))\n",
        "  \n",
        "  return text\n",
        "\n",
        "def _get_mispell(mispell_dict):\n",
        "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
        "    return mispell_dict, mispell_re\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shmrs6ClUm8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kp = KeywordProcessor(case_sensitive=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEdvlp9MU-Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k, v in mispell_dict.items():\n",
        "    kp.add_keyword(k, v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXwwREPDU-Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'(\\&lt)|(\\&gt)', ' url ', text)\n",
        "    \n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n",
        "    text = kp.replace_keywords(text)\n",
        "    text = clean_punct(text)\n",
        "    text = re.sub(r'\\n\\r', ' ', text)\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ja3tPE_U-bE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\n",
        "# train['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\n",
        "# train['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n",
        "\n",
        "# test['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\n",
        "# test['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\n",
        "# test['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw-o2_9SW-3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_masks(tokens, max_seq_length):\n",
        "    \"\"\"Mask for padding\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_segments(tokens, max_seq_length):\n",
        "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        raise IndexError(\"Token length more than max seq length!\")\n",
        "    segments = []\n",
        "    first_sep = True\n",
        "    current_segment_id = 0\n",
        "    for token in tokens:\n",
        "        segments.append(current_segment_id)\n",
        "        if token == \"[SEP]\":\n",
        "            if first_sep:\n",
        "                first_sep = False \n",
        "            else:\n",
        "                current_segment_id = 1\n",
        "    return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "# def _get_segments_v2(tokens, max_seq_length):\n",
        "#     \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
        "#     if len(tokens)>max_seq_length:\n",
        "#         raise IndexError(\"Token length more than max seq length!\")\n",
        "#     segments = []\n",
        "#     current_segment_id = 0\n",
        "#     for token in tokens:\n",
        "#         segments.append(current_segment_id)\n",
        "#         if token == \"[SEP]\":\n",
        "#             current_segment_id += 1\n",
        "#     return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def _get_ids(tokens, tokenizer, max_seq_length):\n",
        "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "    return input_ids\n",
        "\n",
        "def _trim_input(title, question, answer, max_sequence_length, \n",
        "                t_max_len=30, q_max_len=239, a_max_len=239):\n",
        "\n",
        "    t = tokenizer.tokenize(title)\n",
        "    q = tokenizer.tokenize(question)\n",
        "    a = tokenizer.tokenize(answer)\n",
        "    \n",
        "    t_len = len(t)\n",
        "    q_len = len(q)\n",
        "    a_len = len(a)\n",
        "\n",
        "    if (t_len+q_len+a_len+4) > max_sequence_length:\n",
        "        \n",
        "        if t_max_len > t_len:\n",
        "            t_new_len = t_len\n",
        "            a_max_len = a_max_len + floor((t_max_len - t_len)/2)\n",
        "            q_max_len = q_max_len + ceil((t_max_len - t_len)/2)\n",
        "        else:\n",
        "            t_new_len = t_max_len\n",
        "      \n",
        "        if a_max_len > a_len:\n",
        "            a_new_len = a_len \n",
        "            q_new_len = q_max_len + (a_max_len - a_len)\n",
        "        elif q_max_len > q_len:\n",
        "            a_new_len = a_max_len + (q_max_len - q_len)\n",
        "            q_new_len = q_len\n",
        "        else:\n",
        "            a_new_len = a_max_len\n",
        "            q_new_len = q_max_len\n",
        "            \n",
        "            \n",
        "        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n",
        "            raise ValueError(\"New sequence length should be %d, but is %d\" \n",
        "                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n",
        "        \n",
        "        if t_len > t_new_len:\n",
        "            ind1 = floor(t_new_len/2)\n",
        "            ind2 = ceil(t_new_len/2)\n",
        "            t = t[:ind1]+t[-ind2:]\n",
        "        else:\n",
        "            t = t[:t_new_len]\n",
        "\n",
        "        if q_len > q_new_len:\n",
        "            ind1 = floor(q_new_len/2)\n",
        "            ind2 = ceil(q_new_len/2)\n",
        "            q = q[:ind1]+q[-ind2:]\n",
        "        else:\n",
        "            q = q[:q_new_len]\n",
        "\n",
        "        if a_len > a_new_len:\n",
        "            ind1 = floor(a_new_len/2)\n",
        "            ind2 = ceil(a_new_len/2)\n",
        "            a = a[:ind1]+a[-ind2:]\n",
        "        else:\n",
        "            a = a[:a_new_len]\n",
        "    \n",
        "    return t, q, a\n",
        "\n",
        "def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
        "    \n",
        "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
        "\n",
        "    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n",
        "    input_masks = _get_masks(stoken, max_sequence_length)\n",
        "    input_segments = _get_segments(stoken, max_sequence_length)\n",
        "\n",
        "    return [input_ids, input_masks, input_segments]\n",
        "\n",
        "def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows()):\n",
        "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
        "\n",
        "        t, q, a = _trim_input(t, q, a, max_sequence_length)\n",
        "\n",
        "        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "        \n",
        "    return [np.asarray(input_ids, dtype=np.int32), \n",
        "            np.asarray(input_masks, dtype=np.int32), \n",
        "            np.asarray(input_segments, dtype=np.int32)]\n",
        "\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcJFuVXDW-3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "323A8fOJW-3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(torch.utils.data.TensorDataset):\n",
        "\n",
        "    def __init__(self, x_train, idxs, targets=None):\n",
        "        self.input_ids = x_train[0][idxs]\n",
        "        self.input_masks = x_train[1][idxs]\n",
        "        self.input_segments = x_train[2][idxs]\n",
        "        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "#         x_train = self.x_train[idx]\n",
        "        input_ids =  self.input_ids[idx]\n",
        "        input_masks = self.input_masks[idx]\n",
        "        input_segments = self.input_segments[idx]\n",
        "\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        return input_ids, input_masks, input_segments, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ1oU5n_W-3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer,BertConfig,get_linear_schedule_with_warmup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TXqSLKNW-3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained_weights = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCp7_1H-W-3x",
        "colab_type": "code",
        "outputId": "aced5c86-a2b1-4129-94d0-e1e6a806e024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x_train = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "y_train = compute_output_arrays(train, output_categories)\n",
        "x_test = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6079it [00:42, 142.57it/s]\n",
            "476it [00:03, 139.46it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSdXUpPRW-3z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = BertConfig.from_pretrained(pretrained_weights) \n",
        "bert_config.num_labels = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKlp16oXW-31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers import BertPreTrainedModel,BertModel\n",
        "\n",
        "\n",
        "class CustomizedBert(BertPreTrainedModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(CustomizedBert, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        config.output_hidden_states=True\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.linear1 = nn.Linear(config.hidden_size*2, self.config.num_labels)\n",
        "        self.init_weights()\n",
        "\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "\n",
        "        avg_pool = torch.mean(outputs[0], 1)\n",
        "        max_pool, _ = torch.max(outputs[0], 1)\n",
        "\n",
        "        pooled_output = torch.cat((max_pool, avg_pool), 1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        logits = self.linear1(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jYqd5JPW-33",
        "colab_type": "code",
        "outputId": "85158721-936b-46ef-9528-0fc837a758f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "NFOLDS = 3\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 7\n",
        "SEED = 1122\n",
        "num_warmup_steps = 100\n",
        "lr = 2e-5\n",
        "\n",
        "\n",
        "gradient_accumulation_steps = 1\n",
        "seed_everything(SEED)\n",
        "\n",
        "model_list = list()\n",
        "\n",
        "\n",
        "y_oof = np.zeros((len(train), 30))\n",
        "test_pred = np.zeros((len(test), 30))\n",
        "\n",
        "kf = KFold(n_splits=NFOLDS, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "\n",
        "for i, (train_idx, valid_idx) in enumerate(kf.split(x_train[0])):\n",
        "    \n",
        "    \n",
        "    print(f'fold {i+1}')\n",
        "    gc.collect()\n",
        "    \n",
        "    ## loader\n",
        "    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y_train),batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y_train),batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "\n",
        "    t_total = len(train_loader)//gradient_accumulation_steps*EPOCHS\n",
        "\n",
        "\n",
        "    net = CustomizedBert.from_pretrained(pretrained_weights, config=bert_config)\n",
        "    net.cuda()\n",
        "    \n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    optimizer = AdamW(net.parameters(), lr = lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler\n",
        "\n",
        "    for epoch in range(EPOCHS):  \n",
        "        start_time = time.time()\n",
        "        avg_loss = 0.0\n",
        "        net.train()\n",
        "\n",
        "        for step, data in enumerate(train_loader):\n",
        "\n",
        "            # get the inputs\n",
        "            input_ids, input_masks, input_segments, labels = data\n",
        "            pred = net(input_ids = input_ids.long().cuda(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks.cuda(),\n",
        "                             token_type_ids = input_segments.long().cuda(),\n",
        "                            )[0]\n",
        "            \n",
        "            \n",
        "            loss = loss_fn(pred, labels.cuda())\n",
        "        \n",
        "            avg_loss += loss.item()\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "\n",
        "                # Calling the step function on an Optimizer makes an update to its parameters\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                \n",
        "        avg_val_loss = 0.0\n",
        "\n",
        "        valid_preds = np.zeros((len(valid_idx), 30))\n",
        "        true_label = np.zeros((len(valid_idx), 30))\n",
        "        \n",
        "        for j,data in enumerate(val_loader):\n",
        "\n",
        "            # get the inputs\n",
        "            input_ids, input_masks, input_segments, labels = data\n",
        "            pred = net(input_ids = input_ids.long().cuda(),\n",
        "                             labels = None,\n",
        "                             attention_mask = input_masks.cuda(),\n",
        "                             token_type_ids = input_segments.long().cuda(),\n",
        "                            )[0]\n",
        "\n",
        "            loss_val = loss_fn(pred, labels.cuda())\n",
        "            avg_val_loss += loss_val.item()\n",
        "     \n",
        "            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n",
        "            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n",
        "\n",
        "\n",
        "        elapsed_time = time.time() - start_time \n",
        "\n",
        "        score = 0\n",
        "        for i in range(30):\n",
        "          s = np.nan_to_num(\n",
        "                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 30)\n",
        "          score += s\n",
        "\n",
        "        y_oof[valid_idx] = valid_preds\n",
        "\n",
        "        print('Epoch {}/{} \\t loss={:.4f}\\t val_loss={:.4f}\\t spearmanr={:.4f}\\t time={:.2f}s'.format(epoch+1, EPOCHS, avg_loss/len(train_loader),avg_val_loss/len(val_loader),score, elapsed_time))\n",
        "\n",
        "\n",
        "    model_list.append(net)\n",
        "\n",
        "\n",
        "    # result = list()\n",
        "    # with torch.no_grad():\n",
        "    #     for data in test_loader:\n",
        "    #         input_ids, input_masks, input_segments, labels = data\n",
        "    #         y_pred = net(input_ids = input_ids.long().cuda(),\n",
        "    #                             labels = None,\n",
        "    #                             attention_mask = input_masks.cuda(),\n",
        "    #                             token_type_ids = input_segments.long().cuda(),\n",
        "    #                         )[0]\n",
        "    #         result.extend(torch.sigmoid(y_pred).cpu().detach().numpy())\n",
        "            \n",
        "    # test_pred += np.array(result)/NFOLDS\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDCONI_-E4AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# with open(\"/content/drive/My Drive/qa_model/model0112_v2.pkl\",\"wb\") as f:\n",
        "#     pickle.dump(model_list,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWYS2xohfu4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "oof_score = 0\n",
        "for i in range(30):\n",
        "    oof_score += np.nan_to_num(\n",
        "            spearmanr(y_train[:, i], y_oof[:, i]).correlation / 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBQTXsXSfvi2",
        "colab_type": "code",
        "outputId": "d57022c3-9b04-40b4-8011-8049ead54c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Out of folds score = {}\",oof_score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Out of folds score = {} 0.1747905837247968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VViu-mZKW-34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sample_submission.loc[:, output_categories] = test_pred\n",
        "# sample_submission.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW8fLMuBW-36",
        "colab_type": "code",
        "outputId": "8e199e5e-0279-40e3-d4e9-236c40a21be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan  6 14:03:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI4DwGIphUSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = CustomizedBert.from_pretrained(pretrained_weights, config=bert_config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDI70vaCgVri",
        "colab_type": "code",
        "outputId": "4c146022-dd6d-419c-f88e-9ba775d5cf75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "outputs = net(input_ids)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 8, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iml46F4riDHM",
        "colab_type": "code",
        "outputId": "f8a343d8-0eb8-4e08-f1b6-6fd1a0f41326",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outputs[2][-1].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLlbo3UkiEjU",
        "colab_type": "code",
        "outputId": "e9f166ca-8c45-4370-f15c-b79b28254849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "outputs[2][-1][:,0].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1LjrOihiMNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}