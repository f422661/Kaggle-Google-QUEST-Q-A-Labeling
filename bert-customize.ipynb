{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-large-uncased-vocab.txt\n",
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\n",
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-large-uncased/pytorch_model.bin\n",
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-large-uncased/bert_config.json\n",
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased/pytorch_model.bin\n",
      "/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json\n",
      "/kaggle/input/google-quest-challenge/test.csv\n",
      "/kaggle/input/google-quest-challenge/sample_submission.csv\n",
      "/kaggle/input/google-quest-challenge/train.csv\n",
      "/kaggle/input/sacremoses/sacremoses-master/CONTRIBUTORS.md\n",
      "/kaggle/input/sacremoses/sacremoses-master/README.md\n",
      "/kaggle/input/sacremoses/sacremoses-master/.appveyor.yml\n",
      "/kaggle/input/sacremoses/sacremoses-master/requirements.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/setup.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/.travis.yml\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/tokenize.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/subwords.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/util.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/chinese.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/normalize.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/cli.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/__init__.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/truecase.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_truecaser.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_normalizer.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_corpus.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/test/test_tokenizer.py\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lv\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.cs\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ru\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.pl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ca\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.en\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ro\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.zh\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.es\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/README.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ga\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sk\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.it\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.yue\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.el\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.ta\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.hu\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.is\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fr\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.nl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.de\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sl\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.sv\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.fi\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/nonbreaking_prefixes/nonbreaking_prefix.lt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlnum-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Number.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/CJK.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Currency_Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Han.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsSc.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Open_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Line_Separator.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/CJKSymbols.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsSo.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlnum.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Titlecase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsN.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hangul_Syllables.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Katakana.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hiragana.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Close_Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Hangul.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlpha-unichars-au.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsPi.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Symbol.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Punctuation.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Lowercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsUpper.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsLower.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/Uppercase_Letter.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsAlpha.txt\n",
      "/kaggle/input/sacremoses/sacremoses-master/sacremoses/data/perluniprops/IsPf.txt\n",
      "/kaggle/input/bert-model-0103-v2/bert_pytorch_base_folds_5-0103-a.pt\n",
      "/kaggle/input/bert-model-0103-v2/bert_pytorch_base_folds_1-0103-a.pt\n",
      "/kaggle/input/bert-model-0103-v2/bert_pytorch_base_folds_4-0103-a.pt\n",
      "/kaggle/input/bert-model-0103-v2/bert_pytorch_base_folds_2-0103-a.pt\n",
      "/kaggle/input/bert-model-0103-v2/bert_pytorch_base_folds_3-0103-a.pt\n",
      "/kaggle/input/transformers/transformers-master/CONTRIBUTING.md\n",
      "/kaggle/input/transformers/transformers-master/requirements-dev.txt\n",
      "/kaggle/input/transformers/transformers-master/LICENSE\n",
      "/kaggle/input/transformers/transformers-master/README.md\n",
      "/kaggle/input/transformers/transformers-master/deploy_multi_version_doc.sh\n",
      "/kaggle/input/transformers/transformers-master/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/MANIFEST.in\n",
      "/kaggle/input/transformers/transformers-master/setup.py\n",
      "/kaggle/input/transformers/transformers-master/.coveragerc\n",
      "/kaggle/input/transformers/transformers-master/.gitignore\n",
      "/kaggle/input/transformers/transformers-master/hubconf.py\n",
      "/kaggle/input/transformers/transformers-master/docs/README.md\n",
      "/kaggle/input/transformers/transformers-master/docs/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/docs/Makefile\n",
      "/kaggle/input/transformers/transformers-master/docs/source/torchscript.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/migration.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/multilingual.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/converting_tensorflow_models.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/installation.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/benchmarks.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/serialization.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/conf.py\n",
      "/kaggle/input/transformers/transformers-master/docs/source/examples.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/index.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/pretrained_models.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/notebooks.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/quickstart.md\n",
      "/kaggle/input/transformers/transformers-master/docs/source/bertology.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/gpt2.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/distilbert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/xlm.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/transformerxl.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/xlnet.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/bert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/ctrl.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/camembert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/gpt.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/roberta.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/albert.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/model_doc/auto.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/huggingface.css\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/code-snippets.css\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Thin.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Medium.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Regular.otf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/css/Calibre-Light.ttf\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/js/huggingface_logo.svg\n",
      "/kaggle/input/transformers/transformers-master/docs/source/_static/js/custom.js\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_warm_restarts_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_linear_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/transformers_logo_name.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_cosine_hard_restarts_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/imgs/warmup_constant_schedule.png\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/optimizer_schedules.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/tokenizer.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/processors.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/model.rst\n",
      "/kaggle/input/transformers/transformers-master/docs/source/main_classes/configuration.rst\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/utils_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/README.md\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_example_script/run_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/convert_xxx_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/README.md\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/modeling_tf_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tokenization_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/modeling_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/configuration_xxx.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/tokenization_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/modeling_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/templates/adding_a_new_model/tests/modeling_tf_xxx_test.py\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models-MLM-NSP.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-TF-and-PT-models-SQuAD.ipynb\n",
      "/kaggle/input/transformers/transformers-master/notebooks/Comparing-PT-and-TF-models.ipynb\n",
      "/kaggle/input/transformers/transformers-master/.circleci/config.yml\n",
      "/kaggle/input/transformers/transformers-master/.circleci/deploy.sh\n",
      "/kaggle/input/transformers/transformers-master/docker/Dockerfile\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_albert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_beam_search.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_albert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_xlm_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_xlm.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_xlnet_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_bert_pytorch_checkpoint_to_original_tf.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_encoder_decoder.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_pytorch_checkpoint_to_tf2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_openai.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_ctrl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/optimization.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_albert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_transfo_xl_utilities.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_albert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_pytorch_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_tf_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/__main__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_gpt2.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tokenization_roberta.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/convert_openai_original_tf_checkpoint_to_pytorch.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/configuration_albert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_bert.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/file_utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_auto.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/modeling_xlnet.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/glue.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/utils.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/processors/xnli.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/data/metrics/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_openai_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_tests_commons.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_albert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_openai_gpt_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/conftest.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_distilbert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_encoder_decoder_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_albert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_utils_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_ctrl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/__init__.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_transfo_xl_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_roberta_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_xlnet_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_auto_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/configuration_common_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_xlm_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/optimization_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_openai_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_gpt2_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/modeling_tf_bert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/tokenization_albert_test.py\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/sample_text.txt\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/input.txt\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/test_sentencepiece.model\n",
      "/kaggle/input/transformers/transformers-master/transformers/tests/fixtures/spiece.model\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_summarization_test.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_glue.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_squad.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_squad_evaluate.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_multiple_choice.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_summarization.py\n",
      "/kaggle/input/transformers/transformers-master/examples/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/run_lm_finetuning.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_squad.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_tf_glue.py\n",
      "/kaggle/input/transformers/transformers-master/examples/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/examples/run_bertology.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_generation.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_xnli.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_multiple_choice.py\n",
      "/kaggle/input/transformers/transformers-master/examples/test_examples.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_summarization_finetuning.py\n",
      "/kaggle/input/transformers/transformers-master/examples/run_ner.py\n",
      "/kaggle/input/transformers/transformers-master/examples/benchmarks.py\n",
      "/kaggle/input/transformers/transformers-master/examples/utils_ner.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/requirements.txt\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/distiller.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/run_squad_w_distillation.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/train.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/grouped_batch_sampler.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/lm_seqs_dataset.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/utils.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/training_configs/distilgpt2.json\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/training_configs/distilbert-base-uncased.json\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/token_counts.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/extract.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/binarized_data.py\n",
      "/kaggle/input/transformers/transformers-master/examples/distillation/scripts/extract_distilbert.py\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/.gitignore\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/MRPC/dev.tsv\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/MRPC/train.tsv\n",
      "/kaggle/input/transformers/transformers-master/examples/tests_samples/SQUAD/dev-v2.0-small.json\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_transfo_xl.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/README.md\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_openai_gpt.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_swag.py\n",
      "/kaggle/input/transformers/transformers-master/examples/contrib/run_camembert.py\n",
      "/kaggle/input/transformers/transformers-master/.github/stale.yml\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/migration.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/---new-benchmark.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/feature-request.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/--new-model-addition.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/question-help.md\n",
      "/kaggle/input/transformers/transformers-master/.github/ISSUE_TEMPLATE/bug-report.md\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/sacremoses/sacremoses-master\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (1.13.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (0.14.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from sacremoses==0.0.35) (4.39.0)\r\n",
      "Building wheels for collected packages: sacremoses\r\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=882724 sha256=b538eaef3e5e93f28b06b834c1d7dc96c0687b0f9070300472e3e0c8f7a96d33\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/82/48/4b/05cb49d913a40c9d76f97931cd747d72fb17a77b0f6415cdba\r\n",
      "Successfully built sacremoses\r\n",
      "Installing collected packages: sacremoses\r\n",
      "Successfully installed sacremoses-0.0.35\r\n",
      "Processing /kaggle/input/transformers/transformers-master\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (1.17.4)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (1.10.29)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (2.22.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (4.39.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (2019.11.1)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (0.1.83)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==2.2.0) (0.0.35)\r\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.29 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.2.0) (1.13.29)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.2.0) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.2.0) (0.2.1)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.2.0) (2.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.2.0) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.2.0) (2019.9.11)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==2.2.0) (1.24.2)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.2.0) (1.13.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.2.0) (7.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==2.2.0) (0.14.0)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.2.0) (0.15.2)\r\n",
      "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /opt/conda/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.29->boto3->transformers==2.2.0) (2.8.0)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-2.2.0-cp36-none-any.whl size=362783 sha256=aeb120ab7b7eb1863d4ef2386f455872c4a82bf1ae83a7144f4093c487145109\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/f3/1a/ee7248890cb4b8e8975988b1a67999e2d09ef54ce8ee815255\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "Successfully installed transformers-2.2.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/sacremoses/sacremoses-master/\n",
    "!pip install ../input/transformers/transformers-master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import unidecode\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "from flashtext import KeywordProcessor\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import LabelEncoder,MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/google-quest-challenge/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/google-quest-challenge/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/google-quest-challenge/sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr\n",
    "from gensim.models import Word2Vec\n",
    "from flashtext import KeywordProcessor\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel, BertForSequenceClassification, BertConfig,\n",
    "    WEIGHTS_NAME, CONFIG_NAME, AdamW, get_linear_schedule_with_warmup, \n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from math import floor, ceil\n",
    "from transformers import AdamW\n",
    "from transformers.configuration_albert import AlbertConfig\n",
    "from transformers.modeling_bert import ACT2FN, BertEmbeddings, BertSelfAttention, prune_linear_layer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers import AlbertTokenizer, AlbertForSequenceClassification\n",
    "from transformers.optimization import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, DistilBertConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTS = {\n",
    "            '》', '〞', '¢', '‹', '╦', '║', '♪', 'Ø', '╩', '\\\\', '★', '＋', 'ï', '<', '?', '％', '+', '„', 'α', '*', '〰', '｟', '¹', '●', '〗', ']', '▾', '■', '〙', '↓', '´', '【', 'ᴵ',\n",
    "            '\"', '）', '｀', '│', '¤', '²', '‡', '¿', '–', '」', '╔', '〾', '%', '¾', '←', '〔', '＿', '’', '-', ':', '‧', '｛', 'β', '（', '─', 'à', 'â', '､', '•', '；', '☆', '／', 'π',\n",
    "            'é', '╗', '＾', '▪', ',', '►', '/', '〚', '¶', '♦', '™', '}', '″', '＂', '『', '▬', '±', '«', '“', '÷', '×', '^', '!', '╣', '▲', '・', '░', '′', '〝', '‛', '√', ';', '】', '▼',\n",
    "            '.', '~', '`', '。', 'ə', '］', '，', '{', '～', '！', '†', '‘', '﹏', '═', '｣', '〕', '〜', '＼', '▒', '＄', '♥', '〛', '≤', '∞', '_', '[', '＆', '→', '»', '－', '＝', '§', '⋅', \n",
    "            '▓', '&', 'Â', '＞', '〃', '|', '¦', '—', '╚', '〖', '―', '¸', '³', '®', '｠', '¨', '‟', '＊', '£', '#', 'Ã', \"'\", '▀', '·', '？', '、', '█', '”', '＃', '⊕', '=', '〟', '½', '』',\n",
    "            '［', '$', ')', 'θ', '@', '›', '＠', '｝', '¬', '…', '¼', '：', '¥', '❤', '€', '−', '＜', '(', '〘', '▄', '＇', '>', '₤', '₹', '∅', 'è', '〿', '「', '©', '｢', '∙', '°', '｜', '¡', \n",
    "            '↑', 'º', '¯', '♫', '#'\n",
    "          }\n",
    "\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\", \"can't\" : \"cannot\", \"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\", \"didn't\" : \"did not\", \"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\", \"don't\" : \"do not\", \"hadn't\" : \"had not\", \"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\", \"havent\" : \"have not\", \"he'd\" : \"he would\", \"he'll\" : \"he will\", \"he's\" : \"he is\", \"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\", \"i'll\" : \"I will\", \"i'm\" : \"I am\", \"isn't\" : \"is not\", \"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\", \"i've\" : \"I have\", \"let's\" : \"let us\", \"mightn't\" : \"might not\", \"mustn't\" : \"must not\", \n",
    "\"shan't\" : \"shall not\", \"she'd\" : \"she would\", \"she'll\" : \"she will\", \"she's\" : \"she is\", \"shouldn't\" : \"should not\", \"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\", \"thats\" : \"that is\", \"there's\" : \"there is\", \"theres\" : \"there is\", \"they'd\" : \"they would\", \"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\", \"theyre\":  \"they are\", \"they've\" : \"they have\", \"we'd\" : \"we would\", \"we're\" : \"we are\", \"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\", \"what'll\" : \"what will\", \"what're\" : \"what are\", \"what's\" : \"what is\", \"what've\" : \"what have\", \"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\", \"who'll\" : \"who will\", \"who're\" : \"who are\", \"who's\" : \"who is\", \"who've\" : \"who have\", \"won't\" : \"will not\", \"wouldn't\" : \"would not\", \"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\", \"you're\" : \"you are\", \"you've\" : \"you have\", \"'re\": \" are\", \"wasn't\": \"was not\", \"we'll\":\" will\", \"didn't\": \"did not\", \"tryin'\":\"trying\"}\n",
    "\n",
    "\n",
    "def clean_punct(text):\n",
    "    text = str(text)\n",
    "    for punct in PUNCTS:\n",
    "        text = text.replace(punct, ' {} '.format(punct))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp = KeywordProcessor(case_sensitive=True)\n",
    "for k, v in mispell_dict.items():\n",
    "    kp.add_keyword(k, v)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'(\\&lt)|(\\&gt)', ' ', text)\n",
    "    \n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' url ', text)\n",
    "    text = kp.replace_keywords(text)\n",
    "    text = clean_punct(text)\n",
    "    text = re.sub(r'\\n|\\r', ' ', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_title'] = train['question_title'].apply(lambda x : preprocessing(x))\n",
    "train['clean_body'] = train['question_body'].apply(lambda x : preprocessing(x))\n",
    "train['clean_answer'] = train['answer'].apply(lambda x : preprocessing(x))\n",
    "\n",
    "test['clean_title'] = test['question_title'].apply(lambda x : preprocessing(x))\n",
    "test['clean_body'] = test['question_body'].apply(lambda x : preprocessing(x))\n",
    "test['clean_answer'] = test['answer'].apply(lambda x : preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_columns = ['question_asker_intent_understanding',\n",
    "       'question_body_critical', 'question_conversational',\n",
    "       'question_expect_short_answer', 'question_fact_seeking',\n",
    "       'question_has_commonly_accepted_answer',\n",
    "       'question_interestingness_others', 'question_interestingness_self',\n",
    "       'question_multi_intent', 'question_not_really_a_question',\n",
    "       'question_opinion_seeking', 'question_type_choice',\n",
    "       'question_type_compare', 'question_type_consequence',\n",
    "       'question_type_definition', 'question_type_entity',\n",
    "       'question_type_instructions', 'question_type_procedure',\n",
    "       'question_type_reason_explanation', 'question_type_spelling',\n",
    "       'question_well_written', 'answer_helpful',\n",
    "       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n",
    "       'answer_satisfaction', 'answer_type_instructions',\n",
    "       'answer_type_procedure', 'answer_type_reason_explanation',\n",
    "       'answer_well_written']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "find = re.compile(r\"^[^.]*\")\n",
    "\n",
    "train['netloc'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "test['netloc'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n",
    "\n",
    "features = ['netloc', 'category']\n",
    "merged = pd.concat([train[features], test[features]])\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(merged)\n",
    "\n",
    "features_train = ohe.transform(train[features]).toarray()\n",
    "features_test = ohe.transform(test[features]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased-vocab.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6079/6079 [00:35<00:00, 172.86it/s]\n",
      "100%|██████████| 6079/6079 [00:01<00:00, 3161.86it/s]\n",
      "100%|██████████| 476/476 [00:02<00:00, 176.98it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 3099.98it/s]\n",
      "100%|██████████| 6079/6079 [00:32<00:00, 187.18it/s]\n",
      "100%|██████████| 6079/6079 [00:01<00:00, 3253.14it/s]\n",
      "100%|██████████| 476/476 [00:02<00:00, 174.04it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 3144.31it/s]\n"
     ]
    }
   ],
   "source": [
    "def trim_input(tokenizer, title, body):\n",
    "    all_title = []\n",
    "    all_body = []\n",
    "    all_answer = []\n",
    "    for t, b in tqdm(zip(title, body), total=len(title)):\n",
    "        \n",
    "        tokenizer_t = tokenizer.tokenize(t)\n",
    "        tokenizer_b = tokenizer.tokenize(b)\n",
    "        \n",
    "        t_len = len(tokenizer_t)\n",
    "        b_len = len(tokenizer_b)\n",
    "        #a_len = len(tokenizer_a)\n",
    "                \n",
    "        if b_len > BODY_MAX_LEN:\n",
    "            tokenizer_b = tokenizer_b[:BODY_MAX_LEN]\n",
    "            \n",
    "                \n",
    "        if t_len > TITLE_MAX_LEN:\n",
    "            tokenizer_t = tokenizer_t[:TITLE_MAX_LEN]\n",
    "        \n",
    "        all_title.append(tokenizer_t)\n",
    "        all_body.append(tokenizer_b)\n",
    "        \n",
    "    return all_title, all_body\n",
    "\n",
    "def trim_input_a(tokenizer, answer):\n",
    "\n",
    "    all_answer = []\n",
    "    for a in tqdm(answer, total=len(answer)):\n",
    "\n",
    "        tokenizer_a = tokenizer.tokenize(a)\n",
    "\n",
    "        a_len = len(tokenizer_a) \n",
    "        if a_len > ANSWER_MAX_LEN:\n",
    "            tokenizer_a = tokenizer_a[:ANSWER_MAX_LEN]\n",
    "\n",
    "        all_answer.append(tokenizer_a)\n",
    "        \n",
    "    return all_answer\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):\n",
    "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n",
    "    \n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "    return input_ids\n",
    "\n",
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n",
    "    \n",
    "    if len(tokens) > max_seq_length:\n",
    "        raise IndexError(\"Token length more than max seq length!\")\n",
    "        \n",
    "    segments = []\n",
    "    first_sep = True\n",
    "    current_segment_id = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "            if first_sep:\n",
    "                first_sep = False \n",
    "            else:\n",
    "                current_segment_id = 1\n",
    "    return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "\n",
    "def convert_to_bert_inputs(title, question, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n",
    "    \n",
    "    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n",
    "\n",
    "    input_ids = get_ids(stoken, tokenizer, max_sequence_length)\n",
    "    input_masks = get_masks(stoken, max_sequence_length)\n",
    "    input_segments = get_segments(stoken, max_sequence_length)\n",
    "\n",
    "    return [input_ids, input_masks, input_segments]\n",
    "\n",
    "def convert_lines(tokenizer, title, body, max_seq_length=350):\n",
    "    title, body = trim_input(tokenizer, title, body)\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for i, (t, b) in tqdm(enumerate(zip(title, body)), total=len(title)):\n",
    "        stoken = [\"[CLS]\"] + t + [\"[SEP]\"] + b + [\"[SEP]\"]\n",
    "        \n",
    "        ids = get_ids(stoken, tokenizer, max_seq_length)\n",
    "        masks = get_masks(stoken, max_seq_length)\n",
    "        segments = get_segments(stoken, max_seq_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [\n",
    "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "    ]\n",
    "\n",
    "\n",
    "def convert_lines_a(tokenizer, answer, max_seq_length=350):\n",
    "    answer = trim_input_a(tokenizer, answer)\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    \n",
    "    all_tokens = []\n",
    "    input_ids, input_masks, input_segments = [], [], []\n",
    "    for i, a in tqdm(enumerate(answer), total=len(answer)):\n",
    "        stoken = [\"[CLS]\"] + a + [\"[SEP]\"]\n",
    "        \n",
    "        ids = get_ids(stoken, tokenizer, max_seq_length)\n",
    "        masks = get_masks(stoken, max_seq_length)\n",
    "        segments = get_segments(stoken, max_seq_length)\n",
    "        input_ids.append(ids)\n",
    "        input_masks.append(masks)\n",
    "        input_segments.append(segments)\n",
    "        \n",
    "    return [\n",
    "        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n",
    "        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n",
    "        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n",
    "    ]\n",
    "\n",
    "# 29 254\n",
    "TITLE_MAX_LEN = 50\n",
    "BODY_MAX_LEN = 297\n",
    "ANSWER_MAX_LEN = 348\n",
    "\n",
    "x_train_q = convert_lines(tokenizer, train['clean_title'], train['clean_body'])\n",
    "x_test_q = convert_lines(tokenizer, test['clean_title'], test['clean_body'])\n",
    "\n",
    "x_train_a = convert_lines_a(tokenizer, train['clean_answer'])\n",
    "x_test_a = convert_lines_a(tokenizer, test['clean_answer'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6079/6079 [00:00<00:00, 19434.35it/s]\n",
      "100%|██████████| 476/476 [00:00<00:00, 14843.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def add_question_metadata_features(text):\n",
    "    indirect = 0\n",
    "    question_count = 0\n",
    "    reason_explanation_words = 0\n",
    "    choice_words = 0\n",
    "\n",
    "    if '?' in text and '?' == text[-1]: \n",
    "        question_count += 1\n",
    "        for token in text.split():\n",
    "            if token.lower() == 'why':  # question_type_reason_explanation e.g index->102\n",
    "                reason_explanation_words += 1\n",
    "            elif token.lower() == 'or':\n",
    "                choice_words += 1  \n",
    "\n",
    "    if question_count == 0:\n",
    "        indirect = 1\n",
    "    return [indirect, question_count, reason_explanation_words, choice_words]\n",
    "\n",
    "ans_user_category = train[train[['answer_user_name', 'category']].duplicated()][['answer_user_name', 'category']].values.tolist()\n",
    "print(len(ans_user_category))\n",
    "\n",
    "\n",
    "def question_answer_author_same(df):\n",
    "    \n",
    "    q_username = df['question_user_name']\n",
    "    a_username = df['answer_user_name'] \n",
    "    \n",
    "    author_same = []\n",
    "    for i in range(len(df)):\n",
    "        if q_username[i] == a_username[i]:\n",
    "            author_same.append(int(1))\n",
    "        else:\n",
    "            author_same.append(int(0))\n",
    "            \n",
    "    return author_same\n",
    "\n",
    "def count_mark(text):\n",
    "    count = 0\n",
    "    for char in text:\n",
    "        if '?' == char:\n",
    "            count += 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "def add_external_features(df):\n",
    "    \n",
    "    df['clean_title']      = df['clean_title'].apply(lambda x:str(x))\n",
    "    df['title_num_words'] = df.clean_title.str.count('\\S+')\n",
    "\n",
    "    #If the question is longer, it may be more clear, which may help users give a more \n",
    "    df['clean_body']      = df['clean_body'].apply(lambda x:str(x))\n",
    "    df['question_num_words'] = df.clean_body.str.count('\\S+')\n",
    "    \n",
    "    #The assumption here is that longer answer could bring more useful detail\n",
    "    df['clean_answer']            = df['clean_answer'].apply(lambda x:str(x))\n",
    "    df['answer_num_words']  = df.clean_answer.str.count('\\S+')\n",
    "    \n",
    "    #if the question is long and the answer is short, it may be less relevant\n",
    "    df[\"question_vs_answer_length\"] = df['question_num_words'] /  df['answer_num_words']\n",
    "\n",
    "    df[\"total_text_length\"] = df['title_num_words'] + df['question_num_words'] + df['answer_num_words']\n",
    "    \n",
    "    #if answer's author is the same as the corresponding question's author,\n",
    "    #Why he/she asked question.. :)\n",
    "    df[\"q_a_author_same\"] = question_answer_author_same(df)\n",
    "    \n",
    "    df['title_token_len'] = df['clean_title'].apply(lambda x:len(x.split()))\n",
    "    df['body_token_len'] = df['clean_body'].apply(lambda x:len(x.split()))\n",
    "    df['answer_token_len'] = df['clean_answer'].apply(lambda x:len(x.split()))\n",
    "    \n",
    "\n",
    "    df['title_question_mark_count'] = df['clean_title'].apply(lambda x: count_mark(x))\n",
    "    df['body_question_mark_count'] = df['clean_body'].apply(lambda x: count_mark(x))\n",
    "\n",
    "    #answers which was posted by users who answer one category more than one times, they may have read more similar questions.\n",
    "    #thus, the answers by this type of user will more relevent to question.\n",
    "    ans_user_cat = []\n",
    "    for x in tqdm(df[['answer_user_name', 'category']].values.tolist()):\n",
    "        if x in ans_user_category:\n",
    "            ans_user_cat.append(int(1))\n",
    "        else:\n",
    "            ans_user_cat.append(int(0))\n",
    "    df['ans_user_with_cat'] = ans_user_cat\n",
    "    \n",
    "    handmade_features = []\n",
    "\n",
    "    for idx, text in enumerate(df['clean_body'].values):\n",
    "        handmade_features.append(add_question_metadata_features(text))\n",
    "        \n",
    "\n",
    "    return df, np.array(handmade_features)\n",
    "\n",
    "\n",
    "train, train_handmade_features = add_external_features(train)\n",
    "test, test_handmade_features   = add_external_features(test)\n",
    "\n",
    "for c in ['title_num_words', 'question_num_words', 'answer_num_words', 'total_text_length', 'title_token_len', 'body_token_len', 'answer_token_len', 'title_question_mark_count', 'body_question_mark_count']:\n",
    "    minmax = MinMaxScaler()\n",
    "    temp_df = pd.concat([train[[c]], test[[c]]])\n",
    "    minmax.fit(temp_df[c].values.reshape(-1, 1))\n",
    "    train[c]= minmax.transform(train[c].values.reshape(-1, 1))\n",
    "    test[c]= minmax.transform(test[c].values.reshape(-1, 1))\n",
    "\n",
    "additonal_with_content = [\n",
    "   'title_num_words', 'question_num_words', 'answer_num_words',\n",
    "   'question_vs_answer_length', 'total_text_length', 'q_a_author_same',\n",
    "   'title_token_len', 'body_token_len', 'answer_token_len',\n",
    "   'ans_user_with_cat', 'title_question_mark_count',\n",
    "   'body_question_mark_count']\n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.TensorDataset):\n",
    "\n",
    "    def __init__(self, x_train_q, x_train_a,  net_loc, additonal_content_feature, idxs, targets=None):\n",
    "        self.input_ids = x_train_q[0][idxs]\n",
    "        self.input_masks = x_train_q[1][idxs]\n",
    "        self.input_segments = x_train_q[2][idxs]\n",
    "\n",
    "        self.input_ids_a = x_train_a[0][idxs]\n",
    "        self.input_masks_a = x_train_a[1][idxs]\n",
    "        self.input_segments_a = x_train_a[2][idxs]\n",
    "        self.net_loc = net_loc[idxs]\n",
    "        self.additonal_content_feature = additonal_content_feature[idxs]\n",
    "        self.targets = targets[idxs] if targets is not None else np.zeros((x_train_q[0].shape[0], 30))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids =  self.input_ids[idx]\n",
    "        input_masks = self.input_masks[idx]\n",
    "        input_segments = self.input_segments[idx]\n",
    "\n",
    "        input_ids_a =  self.input_ids_a[idx]\n",
    "        input_masks_a = self.input_masks_a[idx]\n",
    "        input_segments_a = self.input_segments_a[idx]\n",
    "        net_loc = self.net_loc[idx]\n",
    "        target = self.targets[idx]\n",
    "        additonal_content_feature = self.additonal_content_feature[idx]\n",
    "        \n",
    "        return input_ids, input_masks, input_segments, input_ids_a, input_masks_a, input_segments_a, net_loc, additonal_content_feature, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulation_steps = 1\n",
    "\n",
    "SEED = 1312\n",
    "NFOLDS = 5\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "LR = 3e-5\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, additional=False):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        \n",
    "        bert_model_config = '/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased/bert_config.json'\n",
    "        \n",
    "        bert_config = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config_a = BertConfig.from_json_file(bert_model_config)\n",
    "        bert_config.num_labels = 21\n",
    "        bert_config_a.num_labels = 9\n",
    "        bert_config.output_hidden_states=True\n",
    "        bert_config_a.output_hidden_states=True\n",
    "        \n",
    "        self.albert = BertModel.from_pretrained('/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config)\n",
    "        self.albert2 = BertModel.from_pretrained('/kaggle/input/bert-pretrained-model/pretrained-bert-models-for-pytorch/bert-base-uncased/', config=bert_config_a)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        #768 1024\n",
    "        self.additional = additional\n",
    "        if self.additional:\n",
    "            self.additional_net_loc_linear = nn.Linear(64, 32)\n",
    "            self.classifier_q = nn.Linear(768*2+32, 21)\n",
    "            self.classifier_a = nn.Linear(768*2+32, 9)\n",
    "        else:\n",
    "            self.classifier_q = nn.Linear(768*2, 21)\n",
    "            self.classifier_a = nn.Linear(768*2, 9)\n",
    "            \n",
    "#         self.init_weights()\n",
    "        torch.nn.init.xavier_normal_(self.classifier_q.weight)\n",
    "        torch.nn.init.xavier_normal_(self.classifier_a.weight)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        input_ids_a=None,\n",
    "        attention_mask_a=None,\n",
    "        token_type_ids_a=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        additional_feature=None,\n",
    "        additional_feature_content=None,\n",
    "    ):\n",
    "        \n",
    "\n",
    "\n",
    "        outputs_q = self.albert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        outputs_a = self.albert2(\n",
    "            input_ids=input_ids_a,\n",
    "            attention_mask=attention_mask_a,\n",
    "            token_type_ids=token_type_ids_a,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "            \n",
    "        hidden_states_q = outputs_q[-1]\n",
    "        hidden_states_a = outputs_a[-1]\n",
    "        \n",
    "        h12_q = hidden_states_q[-1][:, 0].reshape((-1, 1, 768))\n",
    "        h11_q = hidden_states_q[-2][:, 0].reshape((-1, 1, 768))\n",
    "\n",
    "        h12_a = hidden_states_a[-1][:, 0].reshape((-1, 1, 768))\n",
    "        h11_a = hidden_states_a[-2][:, 0].reshape((-1, 1, 768))\n",
    "\n",
    "        all_h_q = torch.cat([h11_q, h12_q], 1)\n",
    "        mean_pool_q = torch.mean(all_h_q, 1)\n",
    "        max_pool_q, _ = torch.max(all_h_q, 1)\n",
    "\n",
    "        all_h_a = torch.cat([h11_a, h12_a], 1)\n",
    "        mean_pool_a = torch.mean(all_h_a, 1)\n",
    "        max_pool_a, _ = torch.max(all_h_a, 1)\n",
    "\n",
    "        pooled_output_q = torch.cat((mean_pool_q, max_pool_q), 1)\n",
    "        pooled_output_a = torch.cat((mean_pool_a, max_pool_a), 1)\n",
    "        \n",
    "        pooled_output_q = self.dropout(pooled_output_q)\n",
    "        pooled_output_a = self.dropout(pooled_output_a)\n",
    "\n",
    "\n",
    "        logits_q = self.classifier_q(pooled_output_q)\n",
    "        logits_a = self.classifier_a(pooled_output_a)\n",
    "\n",
    "        return logits_q, logits_a  # (loss), logits, (hidden_states), (attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Author: Trent J. Bradberry <trentjason@hotmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils.validation import _num_samples, check_array\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits, \\\n",
    "    BaseShuffleSplit, _validate_shuffle_split\n",
    "\n",
    "\n",
    "def IterativeStratification(labels, r, random_state):\n",
    "    \"\"\"This function implements the Iterative Stratification algorithm described\n",
    "    in the following paper:\n",
    "    Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of\n",
    "    Multi-Label Data. In: Gunopulos D., Hofmann T., Malerba D., Vazirgiannis M.\n",
    "    (eds) Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n",
    "    2011. Lecture Notes in Computer Science, vol 6913. Springer, Berlin,\n",
    "    Heidelberg.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = labels.shape[0]\n",
    "    test_folds = np.zeros(n_samples, dtype=int)\n",
    "\n",
    "    # Calculate the desired number of examples at each subset\n",
    "    c_folds = r * n_samples\n",
    "\n",
    "    # Calculate the desired number of examples of each label at each subset\n",
    "    c_folds_labels = np.outer(r, labels.sum(axis=0))\n",
    "\n",
    "    labels_not_processed_mask = np.ones(n_samples, dtype=bool)\n",
    "\n",
    "    while np.any(labels_not_processed_mask):\n",
    "        # Find the label with the fewest (but at least one) remaining examples,\n",
    "        # breaking ties randomly\n",
    "        num_labels = labels[labels_not_processed_mask].sum(axis=0)\n",
    "\n",
    "        # Handle case where only all-zero labels are left by distributing\n",
    "        # across all folds as evenly as possible (not in original algorithm but\n",
    "        # mentioned in the text). (By handling this case separately, some\n",
    "        # code redundancy is introduced; however, this approach allows for\n",
    "        # decreased execution time when there are a relatively large number\n",
    "        # of all-zero labels.)\n",
    "        if num_labels.sum() == 0:\n",
    "            sample_idxs = np.where(labels_not_processed_mask)[0]\n",
    "\n",
    "            for sample_idx in sample_idxs:\n",
    "                fold_idx = np.where(c_folds == c_folds.max())[0]\n",
    "\n",
    "                if fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(fold_idx.shape[0])]\n",
    "\n",
    "                test_folds[sample_idx] = fold_idx\n",
    "                c_folds[fold_idx] -= 1\n",
    "\n",
    "            break\n",
    "\n",
    "        label_idx = np.where(num_labels == num_labels[np.nonzero(num_labels)].min())[0]\n",
    "        if label_idx.shape[0] > 1:\n",
    "            label_idx = label_idx[random_state.choice(label_idx.shape[0])]\n",
    "\n",
    "        sample_idxs = np.where(np.logical_and(labels[:, label_idx].flatten(), labels_not_processed_mask))[0]\n",
    "\n",
    "        for sample_idx in sample_idxs:\n",
    "            # Find the subset(s) with the largest number of desired examples\n",
    "            # for this label, breaking ties by considering the largest number\n",
    "            # of desired examples, breaking further ties randomly\n",
    "            label_folds = c_folds_labels[:, label_idx]\n",
    "            fold_idx = np.where(label_folds == label_folds.max())[0]\n",
    "\n",
    "            if fold_idx.shape[0] > 1:\n",
    "                temp_fold_idx = np.where(c_folds[fold_idx] ==\n",
    "                                         c_folds[fold_idx].max())[0]\n",
    "                fold_idx = fold_idx[temp_fold_idx]\n",
    "\n",
    "                if temp_fold_idx.shape[0] > 1:\n",
    "                    fold_idx = fold_idx[random_state.choice(temp_fold_idx.shape[0])]\n",
    "\n",
    "            test_folds[sample_idx] = fold_idx\n",
    "            labels_not_processed_mask[sample_idx] = False\n",
    "\n",
    "            # Update desired number of examples\n",
    "            c_folds_labels[fold_idx, labels[sample_idx]] -= 1\n",
    "            c_folds[fold_idx] -= 1\n",
    "\n",
    "    return test_folds\n",
    "\n",
    "\n",
    "class MultilabelStratifiedKFold(_BaseKFold):\n",
    "    \"\"\"Multilabel stratified K-Folds cross-validator\n",
    "    Provides train/test indices to split multilabel data into train/test sets.\n",
    "    This cross-validation object is a variation of KFold that returns\n",
    "    stratified folds for multilabel data. The folds are made by preserving\n",
    "    the percentage of samples for each label.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=3\n",
    "        Number of folds. Must be at least 2.\n",
    "    shuffle : boolean, optional\n",
    "        Whether to shuffle each stratification of the data before splitting\n",
    "        into batches.\n",
    "    random_state : int, RandomState instance or None, optional, default=None\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedKFold that only uses random_state\n",
    "        when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> mskf = MultilabelStratifiedKFold(n_splits=2, random_state=0)\n",
    "    >>> mskf.get_n_splits(X, y)\n",
    "    2\n",
    "    >>> print(mskf)  # doctest: +NORMALIZE_WHITESPACE\n",
    "    MultilabelStratifiedKFold(n_splits=2, random_state=0, shuffle=False)\n",
    "    >>> for train_index, test_index in mskf.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different in each fold.\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedMultilabelStratifiedKFold: Repeats Multilabel Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, shuffle=False, random_state=None):\n",
    "        super(MultilabelStratifiedKFold, self).__init__(n_splits, shuffle, random_state)\n",
    "\n",
    "    def _make_test_folds(self, X, y):\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(type_of_target_y))\n",
    "\n",
    "        num_samples = y.shape[0]\n",
    "\n",
    "        rng = check_random_state(self.random_state)\n",
    "        indices = np.arange(num_samples)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rng.shuffle(indices)\n",
    "            y = y[indices]\n",
    "\n",
    "        r = np.asarray([1 / self.n_splits] * self.n_splits)\n",
    "\n",
    "        test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "        return test_folds[np.argsort(indices)]\n",
    "\n",
    "    def _iter_test_masks(self, X=None, y=None, groups=None):\n",
    "        test_folds = self._make_test_folds(X, y)\n",
    "        for i in range(self.n_splits):\n",
    "            yield test_folds == i\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedKFold, self).split(X, y, groups)\n",
    "\n",
    "\n",
    "class RepeatedMultilabelStratifiedKFold(_RepeatedSplits):\n",
    "    \"\"\"Repeated Multilabel Stratified K-Fold cross validator.\n",
    "    Repeats Mulilabel Stratified K-Fold n times with different randomization\n",
    "    in each repetition.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of folds. Must be at least 2.\n",
    "    n_repeats : int, default=10\n",
    "        Number of times cross-validator needs to be repeated.\n",
    "    random_state : None, int or RandomState, default=None\n",
    "        Random state to be used to generate random state for each\n",
    "        repetition as well as randomly breaking ties within the iterative\n",
    "        stratification algorithm.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> rmskf = RepeatedMultilabelStratifiedKFold(n_splits=2, n_repeats=2,\n",
    "    ...     random_state=0)\n",
    "    >>> for train_index, test_index in rmskf.split(X, y):\n",
    "    ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...     X_train, X_test = X[train_index], X[test_index]\n",
    "    ...     y_train, y_test = y[train_index], y[test_index]\n",
    "    ...\n",
    "    TRAIN: [0 3 4 6] TEST: [1 2 5 7]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [0 1 4 5] TEST: [2 3 6 7]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    See also\n",
    "    --------\n",
    "    RepeatedStratifiedKFold: Repeats (Non-multilabel) Stratified K-Fold\n",
    "    n times.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits=5, n_repeats=10, random_state=None):\n",
    "        super(RepeatedMultilabelStratifiedKFold, self).__init__(\n",
    "            MultilabelStratifiedKFold, n_repeats, random_state,\n",
    "            n_splits=n_splits)\n",
    "\n",
    "\n",
    "class MultilabelStratifiedShuffleSplit(BaseShuffleSplit):\n",
    "    \"\"\"Multilabel Stratified ShuffleSplit cross-validator\n",
    "    Provides train/test indices to split data into train/test sets.\n",
    "    This cross-validation object is a merge of MultilabelStratifiedKFold and\n",
    "    ShuffleSplit, which returns stratified randomized folds for multilabel\n",
    "    data. The folds are made by preserving the percentage of each label.\n",
    "    Note: like the ShuffleSplit strategy, multilabel stratified random splits\n",
    "    do not guarantee that all folds will be different, although this is\n",
    "    still very likely for sizeable datasets.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default 10\n",
    "        Number of re-shuffling & splitting iterations.\n",
    "    test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.1.\n",
    "        The default will change in version 0.21. It will remain 0.1 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, default is None\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`. Unlike StratifiedShuffleSplit that only uses\n",
    "        random_state when ``shuffle`` == True, this multilabel implementation\n",
    "        always uses the random_state since the iterative stratification\n",
    "        algorithm breaks ties randomly.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[1,2], [3,4], [1,2], [3,4], [1,2], [3,4], [1,2], [3,4]])\n",
    "    >>> y = np.array([[0,0], [0,0], [0,1], [0,1], [1,1], [1,1], [1,0], [1,0]])\n",
    "    >>> msss = MultilabelStratifiedShuffleSplit(n_splits=3, test_size=0.5,\n",
    "    ...    random_state=0)\n",
    "    >>> msss.get_n_splits(X, y)\n",
    "    3\n",
    "    >>> print(mss)       # doctest: +ELLIPSIS\n",
    "    MultilabelStratifiedShuffleSplit(n_splits=3, random_state=0, test_size=0.5,\n",
    "                                     train_size=None)\n",
    "    >>> for train_index, test_index in msss.split(X, y):\n",
    "    ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    ...    X_train, X_test = X[train_index], X[test_index]\n",
    "    ...    y_train, y_test = y[train_index], y[test_index]\n",
    "    TRAIN: [1 2 5 7] TEST: [0 3 4 6]\n",
    "    TRAIN: [2 3 6 7] TEST: [0 1 4 5]\n",
    "    TRAIN: [1 2 5 6] TEST: [0 3 4 7]\n",
    "    Notes\n",
    "    -----\n",
    "    Train and test sizes may be slightly different from desired due to the\n",
    "    preference of stratification over perfectly sized folds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=10, test_size=\"default\", train_size=None,\n",
    "                 random_state=None):\n",
    "        super(MultilabelStratifiedShuffleSplit, self).__init__(\n",
    "            n_splits, test_size, train_size, random_state)\n",
    "\n",
    "    def _iter_indices(self, X, y, groups=None):\n",
    "        n_samples = _num_samples(X)\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        y = np.asarray(y, dtype=bool)\n",
    "        type_of_target_y = type_of_target(y)\n",
    "\n",
    "        if type_of_target_y != 'multilabel-indicator':\n",
    "            raise ValueError(\n",
    "                'Supported target type is: multilabel-indicator. Got {!r} instead.'.format(\n",
    "                    type_of_target_y))\n",
    "\n",
    "        n_train, n_test = _validate_shuffle_split(n_samples, self.test_size,\n",
    "                                                  self.train_size)\n",
    "\n",
    "        n_samples = y.shape[0]\n",
    "        rng = check_random_state(self.random_state)\n",
    "        y_orig = y.copy()\n",
    "\n",
    "        r = np.array([n_train, n_test]) / (n_train + n_test)\n",
    "\n",
    "        for _ in range(self.n_splits):\n",
    "            indices = np.arange(n_samples)\n",
    "            rng.shuffle(indices)\n",
    "            y = y_orig[indices]\n",
    "\n",
    "            test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\n",
    "\n",
    "            test_idx = test_folds[np.argsort(indices)] == 1\n",
    "            test = np.where(test_idx)[0]\n",
    "            train = np.where(~test_idx)[0]\n",
    "\n",
    "            yield train, test\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "            Note that providing ``y`` is sufficient to generate the splits and\n",
    "            hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
    "            ``X`` instead of actual training data.\n",
    "        y : array-like, shape (n_samples, n_labels)\n",
    "            The target variable for supervised learning problems.\n",
    "            Multilabel stratification is done based on the y labels.\n",
    "        groups : object\n",
    "            Always ignored, exists for compatibility.\n",
    "        Returns\n",
    "        -------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        Notes\n",
    "        -----\n",
    "        Randomized CV splitters may return different results for each call of\n",
    "        split. You can make the results identical by setting ``random_state``\n",
    "        to an integer.\n",
    "        \"\"\"\n",
    "        y = check_array(y, ensure_2d=False, dtype=None)\n",
    "        return super(MultilabelStratifiedShuffleSplit, self).split(X, y, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "additonal_with_content_features_train = np.hstack((train[additonal_with_content].values, train_handmade_features))\n",
    "\n",
    "additonal_with_content_features_test = np.hstack((test[additonal_with_content].values, test_handmade_features))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(TextDataset(x_test_q, x_test_a, features_test, additonal_with_content_features_test, test.index),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 1\n",
      "Epoch 1/2 \t loss=0.7822  \t loss_q=0.3979 \t loss_a=0.3843 \t val_loss=0.7365 \t val_loss_q=0.3700 \t val_loss_a=0.3664 \t  spearman=0.3980 \t time=474.11s\n",
      "Epoch 2/2 \t loss=0.7295  \t loss_q=0.3663 \t loss_a=0.3631 \t val_loss=0.7412 \t val_loss_q=0.3656 \t val_loss_a=0.3756 \t  spearman=0.4000 \t time=472.44s\n",
      "fold 2\n",
      "Epoch 1/2 \t loss=0.7800  \t loss_q=0.3969 \t loss_a=0.3831 \t val_loss=0.7397 \t val_loss_q=0.3740 \t val_loss_a=0.3658 \t  spearman=0.3837 \t time=474.71s\n",
      "Epoch 2/2 \t loss=0.7280  \t loss_q=0.3661 \t loss_a=0.3619 \t val_loss=0.7401 \t val_loss_q=0.3688 \t val_loss_a=0.3713 \t  spearman=0.3901 \t time=473.27s\n",
      "fold 3\n",
      "Epoch 1/2 \t loss=0.7831  \t loss_q=0.3998 \t loss_a=0.3833 \t val_loss=0.7409 \t val_loss_q=0.3674 \t val_loss_a=0.3735 \t  spearman=0.3900 \t time=473.65s\n",
      "Epoch 2/2 \t loss=0.7329  \t loss_q=0.3681 \t loss_a=0.3648 \t val_loss=0.7324 \t val_loss_q=0.3626 \t val_loss_a=0.3697 \t  spearman=0.4036 \t time=473.69s\n",
      "fold 4\n",
      "Epoch 1/2 \t loss=0.7823  \t loss_q=0.3980 \t loss_a=0.3843 \t val_loss=0.7396 \t val_loss_q=0.3696 \t val_loss_a=0.3700 \t  spearman=0.3825 \t time=474.67s\n",
      "Epoch 2/2 \t loss=0.7306  \t loss_q=0.3666 \t loss_a=0.3640 \t val_loss=0.7302 \t val_loss_q=0.3654 \t val_loss_a=0.3649 \t  spearman=0.3963 \t time=474.53s\n",
      "fold 5\n",
      "Epoch 1/2 \t loss=0.7806  \t loss_q=0.3981 \t loss_a=0.3825 \t val_loss=0.7579 \t val_loss_q=0.3765 \t val_loss_a=0.3814 \t  spearman=0.3820 \t time=474.56s\n",
      "Epoch 2/2 \t loss=0.7277  \t loss_q=0.3667 \t loss_a=0.3609 \t val_loss=0.7468 \t val_loss_q=0.3674 \t val_loss_a=0.3794 \t  spearman=0.3984 \t time=474.59s\n"
     ]
    }
   ],
   "source": [
    "y = train.loc[:, y_columns].values\n",
    "\n",
    "oof = np.zeros((len(train), 30))\n",
    "test_pred = np.zeros((len(test), 30))\n",
    "\n",
    "kf = MultilabelStratifiedKFold(n_splits=NFOLDS, random_state=SEED).split(train, y)\n",
    "\n",
    "k = 0\n",
    "for i, (train_idx, valid_idx) in enumerate(kf):\n",
    "    print(f'fold {i+1}')\n",
    "\n",
    "      \n",
    "    gc.collect()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(TextDataset(x_train_q, x_train_a, features_train, additonal_with_content_features_train,  train_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(TextDataset(x_train_q, x_train_a, features_train, additonal_with_content_features_train, valid_idx, y),\n",
    "                          batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    net = BertForSequenceClassification(additional=False)\n",
    "\n",
    "    net.cuda()\n",
    "    # net2.cuda()\n",
    "    \n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    loss_fn2 = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = AdamW(net.parameters(), lr=LR, eps=4e-5)\n",
    "    for epoch in range(EPOCHS):  \n",
    "        start_time = time.time()\n",
    "        avg_loss = 0.0\n",
    "        avg_loss_q = 0.0\n",
    "        avg_loss_a = 0.0\n",
    "        net.train()\n",
    "        # net2.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            # get the inputs\n",
    "            input_ids, input_masks, input_segments, input_ids_a, input_masks_a, input_segments_a, net_features, additional_content, labels = data\n",
    "            pred_q, pred_a  = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                             input_ids_a = input_ids_a.long().cuda(),\n",
    "                             attention_mask_a = input_masks_a.cuda(), \n",
    "                             token_type_ids_a = input_segments_a.cuda(),\n",
    "                            additional_feature = net_features.cuda(),\n",
    "                            )\n",
    "            pred = torch.cat((pred_q, pred_a), 1)\n",
    "            \n",
    "            loss_q = loss_fn(pred_q, labels[:, :21].cuda())\n",
    "            loss_a = loss_fn2(pred_a, labels[:, 21:].cuda())\n",
    "            loss = loss_q + loss_a\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            \n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            avg_loss_q += loss_q.item()\n",
    "            avg_loss_a += loss_a.item()\n",
    "        \n",
    "        avg_val_loss = 0.0\n",
    "        avg_val_loss_q = 0.0\n",
    "        avg_val_loss_a = 0.0\n",
    "        net.eval()\n",
    "\n",
    "        valid_preds = np.zeros((len(valid_idx), 30))\n",
    "        true_label = np.zeros((len(valid_idx), 30))\n",
    "        for j, data in enumerate(val_loader):\n",
    "\n",
    "            # get the inputs\n",
    "            input_ids, input_masks, input_segments, input_ids_a, input_masks_a, input_segments_a, net_features, additional_content, labels = data\n",
    "\n",
    "            ## forward + backward + optimize\n",
    "            \n",
    "            pred_q, pred_a  = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                             input_ids_a = input_ids_a.long().cuda(),\n",
    "                             attention_mask_a = input_masks_a.cuda(), \n",
    "                             token_type_ids_a = input_segments_a.cuda(),\n",
    "                            additional_feature = net_features.cuda(),\n",
    "                            )\n",
    "            pred = torch.cat((pred_q, pred_a), 1)\n",
    "            loss_val_q = loss_fn(pred_q, labels[:, :21].cuda())\n",
    "            loss_val_a = loss_fn2(pred_a, labels[:, 21:].cuda())\n",
    "            loss_val = loss_val_q + loss_val_a\n",
    "            avg_val_loss += loss_val.item()\n",
    "            avg_val_loss_q += loss_val_q.item()\n",
    "            avg_val_loss_a += loss_val_a.item()\n",
    "\n",
    "            \n",
    "            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n",
    "            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n",
    "        \n",
    "        \n",
    "        score = 0\n",
    "        for i in range(30):\n",
    "          s = np.nan_to_num(\n",
    "                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation / 30)\n",
    "          score += s\n",
    "        \n",
    "        \n",
    "        oof[valid_idx] = valid_preds\n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f}  \\t loss_q={:.4f} \\t loss_a={:.4f} \\t val_loss={:.4f} \\t val_loss_q={:.4f} \\t val_loss_a={:.4f} \\t  spearman={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, EPOCHS, avg_loss / len(train_loader), avg_loss_q / len(train_loader), avg_loss_a / len(train_loader), \\\n",
    "             avg_val_loss / len(val_loader), avg_val_loss_q / len(val_loader),  avg_val_loss_a / len(val_loader), score, elapsed_time))\n",
    "        \n",
    "    \n",
    "    k += 1\n",
    "    torch.save(net.state_dict(), \"bert_pytorch_base_folds_{}.pt\".format(k))\n",
    "    net.eval()\n",
    "    test_pred_fold = np.zeros((len(test), 30))\n",
    "    with torch.no_grad():\n",
    "        for q, data in enumerate(test_loader):\n",
    "            input_ids, input_masks, input_segments, input_ids_a, input_masks_a, input_segments_a, net_features, additional_content, labels = data\n",
    "\n",
    "            pred_q, pred_a  = net(input_ids = input_ids.long().cuda(),\n",
    "                             labels = None,\n",
    "                             attention_mask = input_masks.cuda(),\n",
    "                             token_type_ids = input_segments.cuda(),\n",
    "                             input_ids_a = input_ids_a.long().cuda(),\n",
    "                             attention_mask_a = input_masks_a.cuda(), \n",
    "                             token_type_ids_a = input_segments_a.cuda(),\n",
    "                            additional_feature = net_features.cuda(),\n",
    "                            )\n",
    "            y_pred = torch.cat((pred_q, pred_a), 1)\n",
    "            test_pred_fold[q * BATCH_SIZE:(q+1) * BATCH_SIZE] = torch.sigmoid(y_pred).cpu().detach().numpy()\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    test_pred += test_pred_fold/NFOLDS\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score = 0\n",
    "for i in range(30):\n",
    "    oof_score += np.nan_to_num(\n",
    "            spearmanr(y[:, i], oof[:, i]).correlation / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3885292260180297"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.loc[:, y_columns] = test_pred\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>0.943629</td>\n",
       "      <td>0.655591</td>\n",
       "      <td>0.199252</td>\n",
       "      <td>0.527002</td>\n",
       "      <td>0.636663</td>\n",
       "      <td>0.596740</td>\n",
       "      <td>0.666586</td>\n",
       "      <td>0.634873</td>\n",
       "      <td>0.613811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917764</td>\n",
       "      <td>0.915923</td>\n",
       "      <td>0.626021</td>\n",
       "      <td>0.962519</td>\n",
       "      <td>0.962674</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.036153</td>\n",
       "      <td>0.074343</td>\n",
       "      <td>0.796255</td>\n",
       "      <td>0.912743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>0.859836</td>\n",
       "      <td>0.508686</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>0.750617</td>\n",
       "      <td>0.800714</td>\n",
       "      <td>0.909791</td>\n",
       "      <td>0.556808</td>\n",
       "      <td>0.480095</td>\n",
       "      <td>0.127230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688408</td>\n",
       "      <td>0.939088</td>\n",
       "      <td>0.644043</td>\n",
       "      <td>0.965965</td>\n",
       "      <td>0.976975</td>\n",
       "      <td>0.864369</td>\n",
       "      <td>0.908648</td>\n",
       "      <td>0.141205</td>\n",
       "      <td>0.129611</td>\n",
       "      <td>0.923550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>0.917395</td>\n",
       "      <td>0.685009</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>0.722105</td>\n",
       "      <td>0.887749</td>\n",
       "      <td>0.932084</td>\n",
       "      <td>0.596328</td>\n",
       "      <td>0.461238</td>\n",
       "      <td>0.266321</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881696</td>\n",
       "      <td>0.923190</td>\n",
       "      <td>0.600987</td>\n",
       "      <td>0.971232</td>\n",
       "      <td>0.965405</td>\n",
       "      <td>0.814571</td>\n",
       "      <td>0.080409</td>\n",
       "      <td>0.058143</td>\n",
       "      <td>0.905311</td>\n",
       "      <td>0.935257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>0.894433</td>\n",
       "      <td>0.474203</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>0.694094</td>\n",
       "      <td>0.773255</td>\n",
       "      <td>0.900602</td>\n",
       "      <td>0.556111</td>\n",
       "      <td>0.418047</td>\n",
       "      <td>0.126703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748515</td>\n",
       "      <td>0.955547</td>\n",
       "      <td>0.709927</td>\n",
       "      <td>0.968567</td>\n",
       "      <td>0.980789</td>\n",
       "      <td>0.893466</td>\n",
       "      <td>0.744199</td>\n",
       "      <td>0.173645</td>\n",
       "      <td>0.777755</td>\n",
       "      <td>0.914785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>0.909479</td>\n",
       "      <td>0.482548</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.809603</td>\n",
       "      <td>0.795816</td>\n",
       "      <td>0.878340</td>\n",
       "      <td>0.684032</td>\n",
       "      <td>0.582122</td>\n",
       "      <td>0.169670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.676847</td>\n",
       "      <td>0.901841</td>\n",
       "      <td>0.658323</td>\n",
       "      <td>0.961381</td>\n",
       "      <td>0.960251</td>\n",
       "      <td>0.827610</td>\n",
       "      <td>0.280525</td>\n",
       "      <td>0.147515</td>\n",
       "      <td>0.473095</td>\n",
       "      <td>0.928682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>245</td>\n",
       "      <td>0.948966</td>\n",
       "      <td>0.796303</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.639509</td>\n",
       "      <td>0.932208</td>\n",
       "      <td>0.896765</td>\n",
       "      <td>0.629878</td>\n",
       "      <td>0.550286</td>\n",
       "      <td>0.338646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.915386</td>\n",
       "      <td>0.953094</td>\n",
       "      <td>0.658297</td>\n",
       "      <td>0.977129</td>\n",
       "      <td>0.977204</td>\n",
       "      <td>0.882169</td>\n",
       "      <td>0.085993</td>\n",
       "      <td>0.099922</td>\n",
       "      <td>0.822954</td>\n",
       "      <td>0.939190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>257</td>\n",
       "      <td>0.877492</td>\n",
       "      <td>0.507163</td>\n",
       "      <td>0.005759</td>\n",
       "      <td>0.727573</td>\n",
       "      <td>0.759690</td>\n",
       "      <td>0.903613</td>\n",
       "      <td>0.551119</td>\n",
       "      <td>0.410423</td>\n",
       "      <td>0.159648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739054</td>\n",
       "      <td>0.952830</td>\n",
       "      <td>0.697013</td>\n",
       "      <td>0.967430</td>\n",
       "      <td>0.979543</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>0.786493</td>\n",
       "      <td>0.215282</td>\n",
       "      <td>0.645197</td>\n",
       "      <td>0.921336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>267</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.723740</td>\n",
       "      <td>0.165835</td>\n",
       "      <td>0.796666</td>\n",
       "      <td>0.812591</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.655759</td>\n",
       "      <td>0.651067</td>\n",
       "      <td>0.204758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.889887</td>\n",
       "      <td>0.902368</td>\n",
       "      <td>0.680611</td>\n",
       "      <td>0.962273</td>\n",
       "      <td>0.960051</td>\n",
       "      <td>0.816590</td>\n",
       "      <td>0.018601</td>\n",
       "      <td>0.038855</td>\n",
       "      <td>0.825287</td>\n",
       "      <td>0.947138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>284</td>\n",
       "      <td>0.839610</td>\n",
       "      <td>0.394745</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>0.686993</td>\n",
       "      <td>0.764310</td>\n",
       "      <td>0.921893</td>\n",
       "      <td>0.517954</td>\n",
       "      <td>0.428336</td>\n",
       "      <td>0.103370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.695261</td>\n",
       "      <td>0.969559</td>\n",
       "      <td>0.678091</td>\n",
       "      <td>0.979271</td>\n",
       "      <td>0.986497</td>\n",
       "      <td>0.913185</td>\n",
       "      <td>0.629029</td>\n",
       "      <td>0.144782</td>\n",
       "      <td>0.727038</td>\n",
       "      <td>0.930241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>292</td>\n",
       "      <td>0.942840</td>\n",
       "      <td>0.686946</td>\n",
       "      <td>0.018193</td>\n",
       "      <td>0.848419</td>\n",
       "      <td>0.845418</td>\n",
       "      <td>0.916063</td>\n",
       "      <td>0.676750</td>\n",
       "      <td>0.592957</td>\n",
       "      <td>0.147876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853036</td>\n",
       "      <td>0.917736</td>\n",
       "      <td>0.612644</td>\n",
       "      <td>0.960090</td>\n",
       "      <td>0.972590</td>\n",
       "      <td>0.826781</td>\n",
       "      <td>0.634859</td>\n",
       "      <td>0.098095</td>\n",
       "      <td>0.774560</td>\n",
       "      <td>0.921387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   qa_id  question_asker_intent_understanding  question_body_critical  \\\n",
       "0     39                             0.943629                0.655591   \n",
       "1     46                             0.859836                0.508686   \n",
       "2     70                             0.917395                0.685009   \n",
       "3    132                             0.894433                0.474203   \n",
       "4    200                             0.909479                0.482548   \n",
       "5    245                             0.948966                0.796303   \n",
       "6    257                             0.877492                0.507163   \n",
       "7    267                             0.947405                0.723740   \n",
       "8    284                             0.839610                0.394745   \n",
       "9    292                             0.942840                0.686946   \n",
       "\n",
       "   question_conversational  question_expect_short_answer  \\\n",
       "0                 0.199252                      0.527002   \n",
       "1                 0.006255                      0.750617   \n",
       "2                 0.020419                      0.722105   \n",
       "3                 0.006534                      0.694094   \n",
       "4                 0.023460                      0.809603   \n",
       "5                 0.042071                      0.639509   \n",
       "6                 0.005759                      0.727573   \n",
       "7                 0.165835                      0.796666   \n",
       "8                 0.006398                      0.686993   \n",
       "9                 0.018193                      0.848419   \n",
       "\n",
       "   question_fact_seeking  question_has_commonly_accepted_answer  \\\n",
       "0               0.636663                               0.596740   \n",
       "1               0.800714                               0.909791   \n",
       "2               0.887749                               0.932084   \n",
       "3               0.773255                               0.900602   \n",
       "4               0.795816                               0.878340   \n",
       "5               0.932208                               0.896765   \n",
       "6               0.759690                               0.903613   \n",
       "7               0.812591                               0.783927   \n",
       "8               0.764310                               0.921893   \n",
       "9               0.845418                               0.916063   \n",
       "\n",
       "   question_interestingness_others  question_interestingness_self  \\\n",
       "0                         0.666586                       0.634873   \n",
       "1                         0.556808                       0.480095   \n",
       "2                         0.596328                       0.461238   \n",
       "3                         0.556111                       0.418047   \n",
       "4                         0.684032                       0.582122   \n",
       "5                         0.629878                       0.550286   \n",
       "6                         0.551119                       0.410423   \n",
       "7                         0.655759                       0.651067   \n",
       "8                         0.517954                       0.428336   \n",
       "9                         0.676750                       0.592957   \n",
       "\n",
       "   question_multi_intent  ...  question_well_written  answer_helpful  \\\n",
       "0               0.613811  ...               0.917764        0.915923   \n",
       "1               0.127230  ...               0.688408        0.939088   \n",
       "2               0.266321  ...               0.881696        0.923190   \n",
       "3               0.126703  ...               0.748515        0.955547   \n",
       "4               0.169670  ...               0.676847        0.901841   \n",
       "5               0.338646  ...               0.915386        0.953094   \n",
       "6               0.159648  ...               0.739054        0.952830   \n",
       "7               0.204758  ...               0.889887        0.902368   \n",
       "8               0.103370  ...               0.695261        0.969559   \n",
       "9               0.147876  ...               0.853036        0.917736   \n",
       "\n",
       "   answer_level_of_information  answer_plausible  answer_relevance  \\\n",
       "0                     0.626021          0.962519          0.962674   \n",
       "1                     0.644043          0.965965          0.976975   \n",
       "2                     0.600987          0.971232          0.965405   \n",
       "3                     0.709927          0.968567          0.980789   \n",
       "4                     0.658323          0.961381          0.960251   \n",
       "5                     0.658297          0.977129          0.977204   \n",
       "6                     0.697013          0.967430          0.979543   \n",
       "7                     0.680611          0.962273          0.960051   \n",
       "8                     0.678091          0.979271          0.986497   \n",
       "9                     0.612644          0.960090          0.972590   \n",
       "\n",
       "   answer_satisfaction  answer_type_instructions  answer_type_procedure  \\\n",
       "0             0.840686                  0.036153               0.074343   \n",
       "1             0.864369                  0.908648               0.141205   \n",
       "2             0.814571                  0.080409               0.058143   \n",
       "3             0.893466                  0.744199               0.173645   \n",
       "4             0.827610                  0.280525               0.147515   \n",
       "5             0.882169                  0.085993               0.099922   \n",
       "6             0.891300                  0.786493               0.215282   \n",
       "7             0.816590                  0.018601               0.038855   \n",
       "8             0.913185                  0.629029               0.144782   \n",
       "9             0.826781                  0.634859               0.098095   \n",
       "\n",
       "   answer_type_reason_explanation  answer_well_written  \n",
       "0                        0.796255             0.912743  \n",
       "1                        0.129611             0.923550  \n",
       "2                        0.905311             0.935257  \n",
       "3                        0.777755             0.914785  \n",
       "4                        0.473095             0.928682  \n",
       "5                        0.822954             0.939190  \n",
       "6                        0.645197             0.921336  \n",
       "7                        0.825287             0.947138  \n",
       "8                        0.727038             0.930241  \n",
       "9                        0.774560             0.921387  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qa_id</th>\n",
       "      <th>question_asker_intent_understanding</th>\n",
       "      <th>question_body_critical</th>\n",
       "      <th>question_conversational</th>\n",
       "      <th>question_expect_short_answer</th>\n",
       "      <th>question_fact_seeking</th>\n",
       "      <th>question_has_commonly_accepted_answer</th>\n",
       "      <th>question_interestingness_others</th>\n",
       "      <th>question_interestingness_self</th>\n",
       "      <th>question_multi_intent</th>\n",
       "      <th>...</th>\n",
       "      <th>question_well_written</th>\n",
       "      <th>answer_helpful</th>\n",
       "      <th>answer_level_of_information</th>\n",
       "      <th>answer_plausible</th>\n",
       "      <th>answer_relevance</th>\n",
       "      <th>answer_satisfaction</th>\n",
       "      <th>answer_type_instructions</th>\n",
       "      <th>answer_type_procedure</th>\n",
       "      <th>answer_type_reason_explanation</th>\n",
       "      <th>answer_well_written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "      <td>476.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5029.186975</td>\n",
       "      <td>0.894018</td>\n",
       "      <td>0.594608</td>\n",
       "      <td>0.033322</td>\n",
       "      <td>0.714732</td>\n",
       "      <td>0.807849</td>\n",
       "      <td>0.859870</td>\n",
       "      <td>0.586125</td>\n",
       "      <td>0.481539</td>\n",
       "      <td>0.257501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>0.935318</td>\n",
       "      <td>0.668943</td>\n",
       "      <td>0.965038</td>\n",
       "      <td>0.971810</td>\n",
       "      <td>0.865850</td>\n",
       "      <td>0.532813</td>\n",
       "      <td>0.148211</td>\n",
       "      <td>0.525013</td>\n",
       "      <td>0.925178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2812.670060</td>\n",
       "      <td>0.044599</td>\n",
       "      <td>0.135410</td>\n",
       "      <td>0.060157</td>\n",
       "      <td>0.093191</td>\n",
       "      <td>0.087943</td>\n",
       "      <td>0.107971</td>\n",
       "      <td>0.050150</td>\n",
       "      <td>0.079771</td>\n",
       "      <td>0.195531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085808</td>\n",
       "      <td>0.029066</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>0.014417</td>\n",
       "      <td>0.042336</td>\n",
       "      <td>0.303631</td>\n",
       "      <td>0.053958</td>\n",
       "      <td>0.265537</td>\n",
       "      <td>0.023215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>0.757599</td>\n",
       "      <td>0.327065</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.270911</td>\n",
       "      <td>0.464504</td>\n",
       "      <td>0.275191</td>\n",
       "      <td>0.493716</td>\n",
       "      <td>0.346994</td>\n",
       "      <td>0.023676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627161</td>\n",
       "      <td>0.717092</td>\n",
       "      <td>0.523565</td>\n",
       "      <td>0.871497</td>\n",
       "      <td>0.883014</td>\n",
       "      <td>0.579397</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>0.013141</td>\n",
       "      <td>0.044694</td>\n",
       "      <td>0.733869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2572.000000</td>\n",
       "      <td>0.864836</td>\n",
       "      <td>0.475272</td>\n",
       "      <td>0.006857</td>\n",
       "      <td>0.660560</td>\n",
       "      <td>0.770251</td>\n",
       "      <td>0.836549</td>\n",
       "      <td>0.543666</td>\n",
       "      <td>0.424493</td>\n",
       "      <td>0.103290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725241</td>\n",
       "      <td>0.923306</td>\n",
       "      <td>0.639318</td>\n",
       "      <td>0.960424</td>\n",
       "      <td>0.967553</td>\n",
       "      <td>0.847923</td>\n",
       "      <td>0.274871</td>\n",
       "      <td>0.110155</td>\n",
       "      <td>0.298145</td>\n",
       "      <td>0.915471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5093.000000</td>\n",
       "      <td>0.896693</td>\n",
       "      <td>0.576770</td>\n",
       "      <td>0.010710</td>\n",
       "      <td>0.710736</td>\n",
       "      <td>0.812723</td>\n",
       "      <td>0.899376</td>\n",
       "      <td>0.578074</td>\n",
       "      <td>0.453199</td>\n",
       "      <td>0.198185</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798645</td>\n",
       "      <td>0.941950</td>\n",
       "      <td>0.670256</td>\n",
       "      <td>0.967397</td>\n",
       "      <td>0.975853</td>\n",
       "      <td>0.875755</td>\n",
       "      <td>0.588666</td>\n",
       "      <td>0.153303</td>\n",
       "      <td>0.522841</td>\n",
       "      <td>0.930653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7482.000000</td>\n",
       "      <td>0.929720</td>\n",
       "      <td>0.709907</td>\n",
       "      <td>0.026951</td>\n",
       "      <td>0.768440</td>\n",
       "      <td>0.859959</td>\n",
       "      <td>0.922874</td>\n",
       "      <td>0.621664</td>\n",
       "      <td>0.522915</td>\n",
       "      <td>0.378538</td>\n",
       "      <td>...</td>\n",
       "      <td>0.876928</td>\n",
       "      <td>0.954027</td>\n",
       "      <td>0.699697</td>\n",
       "      <td>0.973008</td>\n",
       "      <td>0.980805</td>\n",
       "      <td>0.894699</td>\n",
       "      <td>0.811294</td>\n",
       "      <td>0.185857</td>\n",
       "      <td>0.773146</td>\n",
       "      <td>0.940930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9640.000000</td>\n",
       "      <td>0.972550</td>\n",
       "      <td>0.882987</td>\n",
       "      <td>0.474059</td>\n",
       "      <td>0.953217</td>\n",
       "      <td>0.979043</td>\n",
       "      <td>0.964814</td>\n",
       "      <td>0.722616</td>\n",
       "      <td>0.748445</td>\n",
       "      <td>0.823458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.949123</td>\n",
       "      <td>0.978208</td>\n",
       "      <td>0.775337</td>\n",
       "      <td>0.984264</td>\n",
       "      <td>0.988438</td>\n",
       "      <td>0.947534</td>\n",
       "      <td>0.953878</td>\n",
       "      <td>0.267514</td>\n",
       "      <td>0.976193</td>\n",
       "      <td>0.960213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             qa_id  question_asker_intent_understanding  \\\n",
       "count   476.000000                           476.000000   \n",
       "mean   5029.186975                             0.894018   \n",
       "std    2812.670060                             0.044599   \n",
       "min      39.000000                             0.757599   \n",
       "25%    2572.000000                             0.864836   \n",
       "50%    5093.000000                             0.896693   \n",
       "75%    7482.000000                             0.929720   \n",
       "max    9640.000000                             0.972550   \n",
       "\n",
       "       question_body_critical  question_conversational  \\\n",
       "count              476.000000               476.000000   \n",
       "mean                 0.594608                 0.033322   \n",
       "std                  0.135410                 0.060157   \n",
       "min                  0.327065                 0.003384   \n",
       "25%                  0.475272                 0.006857   \n",
       "50%                  0.576770                 0.010710   \n",
       "75%                  0.709907                 0.026951   \n",
       "max                  0.882987                 0.474059   \n",
       "\n",
       "       question_expect_short_answer  question_fact_seeking  \\\n",
       "count                    476.000000             476.000000   \n",
       "mean                       0.714732               0.807849   \n",
       "std                        0.093191               0.087943   \n",
       "min                        0.270911               0.464504   \n",
       "25%                        0.660560               0.770251   \n",
       "50%                        0.710736               0.812723   \n",
       "75%                        0.768440               0.859959   \n",
       "max                        0.953217               0.979043   \n",
       "\n",
       "       question_has_commonly_accepted_answer  question_interestingness_others  \\\n",
       "count                             476.000000                       476.000000   \n",
       "mean                                0.859870                         0.586125   \n",
       "std                                 0.107971                         0.050150   \n",
       "min                                 0.275191                         0.493716   \n",
       "25%                                 0.836549                         0.543666   \n",
       "50%                                 0.899376                         0.578074   \n",
       "75%                                 0.922874                         0.621664   \n",
       "max                                 0.964814                         0.722616   \n",
       "\n",
       "       question_interestingness_self  question_multi_intent  ...  \\\n",
       "count                     476.000000             476.000000  ...   \n",
       "mean                        0.481539               0.257501  ...   \n",
       "std                         0.079771               0.195531  ...   \n",
       "min                         0.346994               0.023676  ...   \n",
       "25%                         0.424493               0.103290  ...   \n",
       "50%                         0.453199               0.198185  ...   \n",
       "75%                         0.522915               0.378538  ...   \n",
       "max                         0.748445               0.823458  ...   \n",
       "\n",
       "       question_well_written  answer_helpful  answer_level_of_information  \\\n",
       "count             476.000000      476.000000                   476.000000   \n",
       "mean                0.796661        0.935318                     0.668943   \n",
       "std                 0.085808        0.029066                     0.042677   \n",
       "min                 0.627161        0.717092                     0.523565   \n",
       "25%                 0.725241        0.923306                     0.639318   \n",
       "50%                 0.798645        0.941950                     0.670256   \n",
       "75%                 0.876928        0.954027                     0.699697   \n",
       "max                 0.949123        0.978208                     0.775337   \n",
       "\n",
       "       answer_plausible  answer_relevance  answer_satisfaction  \\\n",
       "count        476.000000        476.000000           476.000000   \n",
       "mean           0.965038          0.971810             0.865850   \n",
       "std            0.012574          0.014417             0.042336   \n",
       "min            0.871497          0.883014             0.579397   \n",
       "25%            0.960424          0.967553             0.847923   \n",
       "50%            0.967397          0.975853             0.875755   \n",
       "75%            0.973008          0.980805             0.894699   \n",
       "max            0.984264          0.988438             0.947534   \n",
       "\n",
       "       answer_type_instructions  answer_type_procedure  \\\n",
       "count                476.000000             476.000000   \n",
       "mean                   0.532813               0.148211   \n",
       "std                    0.303631               0.053958   \n",
       "min                    0.004074               0.013141   \n",
       "25%                    0.274871               0.110155   \n",
       "50%                    0.588666               0.153303   \n",
       "75%                    0.811294               0.185857   \n",
       "max                    0.953878               0.267514   \n",
       "\n",
       "       answer_type_reason_explanation  answer_well_written  \n",
       "count                      476.000000           476.000000  \n",
       "mean                         0.525013             0.925178  \n",
       "std                          0.265537             0.023215  \n",
       "min                          0.044694             0.733869  \n",
       "25%                          0.298145             0.915471  \n",
       "50%                          0.522841             0.930653  \n",
       "75%                          0.773146             0.940930  \n",
       "max                          0.976193             0.960213  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
